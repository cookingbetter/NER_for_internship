{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "do_not_touch2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Hi0WxCmZ3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1c1dc1-46a5-4de2-95c3-f274a6aae8dd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n",
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-24 19:36:22--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3281528 (3.1M) [text/plain]\n",
            "Saving to: ‘eng.train’\n",
            "\n",
            "\reng.train             0%[                    ]       0  --.-KB/s               \reng.train           100%[===================>]   3.13M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-24 19:36:22 (59.1 MB/s) - ‘eng.train’ saved [3281528/3281528]\n",
            "\n",
            "--2021-05-24 19:36:22--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827012 (808K) [text/plain]\n",
            "Saving to: ‘eng.testa’\n",
            "\n",
            "eng.testa           100%[===================>] 807.63K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-24 19:36:22 (31.6 MB/s) - ‘eng.testa’ saved [827012/827012]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjycRDHMmZ5K"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy18jL-UmZ8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb78d65-9c81-4e01-ef1f-181408e94ff3"
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 26.2MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f5/06e099d85c66c68f203b356117e9df68e98983f2120b9ae598dc840c20e2/boto3-1.17.79-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.79\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e5/f49beffe2474490a5e7811d533049a4fb701a6f87c66e55724a6b11c25e2/botocore-1.20.79-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 44.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.79->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.79->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.79 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.79 botocore-1.20.79 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB_yJP_8yvVU",
        "outputId": "52bcb35c-e1b7-4030-b4e3-f9c041373bdd"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "VOCAB = ('<PAD>', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG')\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
        "\n",
        "class NerDataset(data.Dataset):\n",
        "    def __init__(self, fpath):\n",
        "\n",
        "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
        "        sents, tags_li = [], []\n",
        "        for entry in entries:\n",
        "            words = [line.split()[0] for line in entry.splitlines()]\n",
        "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx]\n",
        "\n",
        "        x, y = [], []\n",
        "        is_heads = []\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)\n",
        "            yy = [tag2idx[each] for each in t]\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
        "\n",
        "        seqlen = len(y)\n",
        "\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "\n",
        "\n",
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 21560922.59B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgcSoTVHyvZm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=None, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, y, ):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        if self.training:\n",
        "            self.bert.train()\n",
        "            encoded_layers, _ = self.bert(x)\n",
        "            enc = encoded_layers[-1]\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "\n",
        "        logits = self.fc(enc)\n",
        "        y_hat = logits.argmax(-1)\n",
        "        return logits, y, y_hat"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw-wqqcvyvb9"
      },
      "source": [
        "batch_size = 32\n",
        "lr = 1e-4\n",
        "n_epochs = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEcRLp5ryvd3",
        "outputId": "9609fe7e-e1e6-4bc2-9ef5-028511dd344c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "        y = y.view(-1)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i==0:\n",
        "            print(\"=====sanity check======\")\n",
        "            print(\"words:\", words[0])\n",
        "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
        "            print(\"is_heads:\", is_heads[0])\n",
        "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tags:\", tags[0])\n",
        "            print(\"seqlen:\", seqlens[0])\n",
        "            print(\"=======================\")\n",
        "\n",
        "\n",
        "        if i%25==0:\n",
        "            print(f\"step: {i}, loss: {loss.item()}\")\n",
        "\n",
        "def eval(model, iterator):\n",
        "    model.eval()\n",
        "\n",
        "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "            _, _, y_hat = model(x, y)\n",
        "\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "            Tags.extend(tags)\n",
        "            Y.extend(y.numpy().tolist())\n",
        "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "    with open(\"temp\", 'w') as fout:\n",
        "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "            preds = [idx2tag[hat] for hat in y_hat]\n",
        "            assert len(preds)==len(words.split())==len(tags.split())\n",
        "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
        "                fout.write(f\"{w} {t} {p}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    y_true =  np.array([tag2idx[line.split()[1]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
        "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
        "\n",
        "    num_proposed = len(y_pred[y_pred>1])\n",
        "    num_correct = (np.logical_and(y_true==y_pred, y_true>1)).astype(np.int).sum()\n",
        "    num_gold = len(y_true[y_true>1])\n",
        "\n",
        "    print(f\"num_proposed:{num_proposed}\")\n",
        "    print(f\"num_correct:{num_correct}\")\n",
        "    print(f\"num_gold:{num_gold}\")\n",
        "\n",
        "    precision = num_correct / num_proposed\n",
        "    recall = num_correct / num_gold\n",
        "    f1 = 2*precision*recall / (precision + recall)\n",
        "            \n",
        "    os.remove(\"temp\")\n",
        "\n",
        "    print(\"precision=%.2f\"%precision)\n",
        "    print(\"recall=%.2f\"%recall)\n",
        "    print(\"f1=%.2f\"%f1)\n",
        "    return precision, recall, f1\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    model = Net(len(VOCAB), device).cuda()\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "    train_dataset = NerDataset('eng.train')\n",
        "    eval_dataset = NerDataset('eng.testa')\n",
        "\n",
        "    train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=True, \n",
        "                                 num_workers=4,\n",
        "                                 collate_fn=pad)\n",
        "    eval_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=4,\n",
        "                                 collate_fn=pad)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        train(model, train_iter, optimizer, criterion)\n",
        "\n",
        "        print(f\"=========eval at epoch={epoch}=========\")\n",
        "        precision, recall, f1 = eval(model, eval_iter)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 404400730/404400730 [00:07<00:00, 51729088.91B/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=====sanity check======\n",
            "words: [CLS] At Rio , they joined up with the national team squad for the journey to Moscow , where Brazil will face Russia in a friendly international on Wednesday . [SEP]\n",
            "x: [ 101 1335 5470  117 1152 1688 1146 1114 1103 1569 1264 4322 1111 1103\n",
            " 5012 1106 4116  117 1187 3524 1209 1339 2733 1107  170 4931 1835 1113\n",
            " 9031  119  102]\n",
            "tokens: ['[CLS]', 'At', 'Rio', ',', 'they', 'joined', 'up', 'with', 'the', 'national', 'team', 'squad', 'for', 'the', 'journey', 'to', 'Moscow', ',', 'where', 'Brazil', 'will', 'face', 'Russia', 'in', 'a', 'friendly', 'international', 'on', 'Wednesday', '.', '[SEP]']\n",
            "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "y: [0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 0]\n",
            "tags: <PAD> O I-LOC O O O O O O O O O O O O O I-LOC O O I-LOC O O I-LOC O O O O O O O <PAD>\n",
            "seqlen: 31\n",
            "=======================\n",
            "step: 0, loss: 2.2725512981414795\n",
            "step: 25, loss: 0.09014217555522919\n",
            "step: 50, loss: 0.09056064486503601\n",
            "step: 75, loss: 0.10565365105867386\n",
            "step: 100, loss: 0.0880051925778389\n",
            "step: 125, loss: 0.042356640100479126\n",
            "step: 150, loss: 0.03453563526272774\n",
            "step: 175, loss: 0.019040578976273537\n",
            "step: 200, loss: 0.10462163388729095\n",
            "step: 225, loss: 0.013219919055700302\n",
            "step: 250, loss: 0.05119611322879791\n",
            "step: 275, loss: 0.03399975225329399\n",
            "step: 300, loss: 0.03936026617884636\n",
            "step: 325, loss: 0.05346162989735603\n",
            "step: 350, loss: 0.014337067492306232\n",
            "step: 375, loss: 0.04135572910308838\n",
            "step: 400, loss: 0.026915278285741806\n",
            "step: 425, loss: 0.04587573558092117\n",
            "step: 450, loss: 0.06991571933031082\n",
            "=========eval at epoch=1=========\n",
            "num_proposed:8714\n",
            "num_correct:8180\n",
            "num_gold:8603\n",
            "precision=0.94\n",
            "recall=0.95\n",
            "f1=0.94\n",
            "=====sanity check======\n",
            "words: [CLS] \" Salvador is a small facility , and the prospects are that if there will be a strike , it will not be a long strike , \" O'Neill said . [SEP]\n",
            "x: [  101   107 10479  1110   170  1353  3695   117  1105  1103 19743  1132\n",
            "  1115  1191  1175  1209  1129   170  4585   117  1122  1209  1136  1129\n",
            "   170  1263  4585   117   107   152   112 11935  1163   119   102]\n",
            "tokens: ['[CLS]', '\"', 'Salvador', 'is', 'a', 'small', 'facility', ',', 'and', 'the', 'prospects', 'are', 'that', 'if', 'there', 'will', 'be', 'a', 'strike', ',', 'it', 'will', 'not', 'be', 'a', 'long', 'strike', ',', '\"', 'O', \"'\", 'Neill', 'said', '.', '[SEP]']\n",
            "is_heads: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
            "y: [0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 0 0 1 1 0]\n",
            "tags: <PAD> O I-LOC O O O O O O O O O O O O O O O O O O O O O O O O O O I-PER O O <PAD>\n",
            "seqlen: 35\n",
            "=======================\n",
            "step: 0, loss: 0.07764995098114014\n",
            "step: 25, loss: 0.025155622512102127\n",
            "step: 50, loss: 0.006865320261567831\n",
            "step: 75, loss: 0.059751562774181366\n",
            "step: 100, loss: 0.05902634561061859\n",
            "step: 125, loss: 0.016630196943879128\n",
            "step: 150, loss: 0.037760380655527115\n",
            "step: 175, loss: 0.048715267330408096\n",
            "step: 200, loss: 0.004711574409157038\n",
            "step: 225, loss: 0.0047507379204034805\n",
            "step: 250, loss: 0.015830589458346367\n",
            "step: 275, loss: 0.05111178755760193\n",
            "step: 300, loss: 0.009185289964079857\n",
            "step: 325, loss: 0.036535002291202545\n",
            "step: 350, loss: 0.0194820836186409\n",
            "step: 375, loss: 0.08313782513141632\n",
            "step: 400, loss: 0.06122384965419769\n",
            "step: 425, loss: 0.03778120130300522\n",
            "step: 450, loss: 0.0039085885509848595\n",
            "=========eval at epoch=2=========\n",
            "num_proposed:8597\n",
            "num_correct:8215\n",
            "num_gold:8603\n",
            "precision=0.96\n",
            "recall=0.95\n",
            "f1=0.96\n",
            "=====sanity check======\n",
            "words: [CLS] Painted parrot scam lands Australian in jail . [SEP]\n",
            "x: [  101 13304  1906 14247 10595   188 24282  4508  1925  1107  7237   119\n",
            "   102]\n",
            "tokens: ['[CLS]', 'Pain', '##ted', 'par', '##rot', 's', '##cam', 'lands', 'Australian', 'in', 'jail', '.', '[SEP]']\n",
            "is_heads: [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
            "y: [0 1 0 1 0 1 0 1 6 1 1 1 0]\n",
            "tags: <PAD> O O O O I-MISC O O O <PAD>\n",
            "seqlen: 13\n",
            "=======================\n",
            "step: 0, loss: 0.006316464859992266\n",
            "step: 25, loss: 0.0030532795935869217\n",
            "step: 50, loss: 0.0028771646320819855\n",
            "step: 75, loss: 0.01262825820595026\n",
            "step: 100, loss: 0.003182818880304694\n",
            "step: 125, loss: 0.0025507390964776278\n",
            "step: 150, loss: 0.004243437200784683\n",
            "step: 175, loss: 0.022088907659053802\n",
            "step: 200, loss: 0.011122462339699268\n",
            "step: 225, loss: 0.011518807150423527\n",
            "step: 250, loss: 0.009017404168844223\n",
            "step: 275, loss: 0.027197951450943947\n",
            "step: 300, loss: 0.01549282856285572\n",
            "step: 325, loss: 0.012134794145822525\n",
            "step: 350, loss: 0.02856043353676796\n",
            "step: 375, loss: 0.0030500704888254404\n",
            "step: 400, loss: 0.015554272569715977\n",
            "step: 425, loss: 0.010452319867908955\n",
            "step: 450, loss: 0.017524372786283493\n",
            "=========eval at epoch=3=========\n",
            "num_proposed:8660\n",
            "num_correct:8180\n",
            "num_gold:8603\n",
            "precision=0.94\n",
            "recall=0.95\n",
            "f1=0.95\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}